<div class="modal-header">
    <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
    <h4 class="modal-title">Sicherheitslücken in der künstlichen Intelligenz</h4>
    <p>
        <i>Konrad Rieck</i>
    </p>
</div>
<div class="modal-body"><div class="te">
    <p>
        Techniken des maschinellen Lernens und der künstlichen Intelligenz
werden zunehmend in sicherheitskritischen Anwendungen eingesetzt, wie
zum Beispiel in autonomen Fahrzeugen und Robotern. Aktuelle
Lernalgorithmen sind jedoch häufig verwundbar für Angriffe und können
durch geschickte Eingaben getäuscht werden. Diese Angriffen können
beispielsweise den Lernvorgang stören (Poisoning), Vorhersagen
verfälschen (Evasion) oder vertrauliche Informationen extrahieren
(Model Stealing). Obwohl lernende Systeme bereits in viele Produkte
integriert werden, hat die Forschung gerade erst begonnen sich mit
diesem Problem auseinanderzusetzen. Der Vortrag stellt verschiedene
Arten von Angriffen gegen Lernalgorithmen vor und diskutiert ihre
technischen Grundlagen anhand von Beispielen.  Der Vortrag schließt
mit einem Ausblick auf Verteidigungsmaßnahmen aus der aktuellen
Forschung.
    </p>
</div>
<div class="te">
    <p>
        <i>Bio</i>
    </p>
    <p>
    </p>
</div></div>
<div class="modal-footer">
    <button type="button" class="btn btn-default" data-dismiss="modal">Schließen</button>
</div>
